# Experimenting CNN Components
> A journey to understand what actually happens inside a Convolutional Neural Network.

I was confused about CNN components and wanted to actually **understand** them. So I built these experiments to answer the questions that bugged me.

## â“ The Questions
* Does adding more layers always improve accuracy?
* How much do feature maps actually matter?
* What's the "optimal" architecture for MNIST?

## ğŸ› ï¸ My Approach
* âœ… **Controlled experiments:** Changing only one variable at a time to isolate impact.
* âœ… **Reproducibility:** Used fixed seeds so you can get the same results.
* âœ… **Visual Analysis:** Clear documentation of training curves and results.
* âœ… **Honest Analysis:** Documenting what works and, more importantly, what doesn't.

## ğŸŒŸ What's Different Here?
I try to tested multiple configurations and documented surprisesâ€”**like why 2 layers sometimes beat 3!** I wanted to move beyond "just following along" to truly understanding the architecture.

## ğŸ“š Acknowledgments
This work builds upon excellent tutorials and insights from:
- [How to Choose CNN Architecture - MNIST](https://www.kaggle.com/code/cdeotte/how-to-choose-cnn-architecture-mnist) by Chris Deotte
- [Convolutional Neural Network Tutorial](https://www.kaggle.com/code/kanncaa1/convolutional-neural-network-cnn-tutorial) by Kaan

---
âš ï¸ **Disclaimer:** This is a learning project. Some models may exhibit overfitting as the focus is on understanding the **relative impact** of components. Regularization techniques like dropout are explored in later sections.
